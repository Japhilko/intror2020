---
title: "Generalized Linear Models with R"
author: "Jan-Philipp Kolb"
date: "`r format(Sys.time(), '%d %B, %Y')`"
fontsize: 10pt
output:
  beamer_presentation: 
    fig_caption: no
    fig_height: 3
    fig_width: 5
    fonttheme: structuresmallcapsserif
    highlight: zenburn
    colortheme: whale
    theme: Warsaw
  pdf_document:
    toc: yes
  html_document:
    keep_md: yes
  slidy_presentation:
    keep_md: yes
---


```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=F,message=F,eval=T)
```

```{r,echo=F}
showsolut=F
```



```{r,echo=F}
dat <- readstata13::read.dta13("../data/ZA5666_v1-0-0_Stata14.dta")
datf <- readstata13::read.dta13("../data/ZA5666_v1-0-0_Stata14.dta",
                  convert.factors = F)

```


## Agresti - [Categorical Data Analysis
 (2002)](https://mathdept.iut.ac.ir/sites/mathdept.iut.ac.ir/files/AGRESTI.PDF)

![](figure/CDAagresti.PNG)

- Very intuitively written book
- Very detailed accompanying script by [**Laura A. Thompson**](http://statweb.stanford.edu/~owen/courses/306a/Splusdiscrete2.pdf)
- The paper deals with categorical data analysis in general.

## Faraway books on regression with R

![](figure/extendinglinearmodel.PNG)

-  Logistic regression intuitively explained
-  Examples with R-code
    - Faraway - [**Extending the linear model with R**](http://www.maths.bath.ac.uk/~jjf23/ELM/scripts2/index.html)
    - Faraway - [**Practical Regression and Anova using R**](https://cran.r-project.org/doc/contrib/Faraway-PRA.pdf)
    
## Import the GESIS Panel dataset again

```{r,eval=F}
library(readstata13)
datf <- read.dta13("../data/ZA5666_v1-0-0_Stata14.dta",
                  convert.factors = F)
```

The argument `convert.factors`:

- logical. If TRUE, factors from Stata value labels are created.

## A function to recode the missing values

```{r,echo=T}
transform_missings <- function(var){
  misvals <- c(-11,-22,-33,-44,-55,-66,-77,-88,-99,-111)
  var[var %in% misvals] <- NA
  return(var)
}
```


## Variables for `glm`'s

- a11d056z: age group 

```{r}
table(datf$a11d056z)
```


```{r}
age <- transform_missings(datf$a11d056z)
```


```{r}
table(age)
```


## GP variable a11d094a: Children under 16 years

Does your household include children under 16?

- 1 Yes
- 2 No

```{r}
children <- as.factor(transform_missings(datf$a11d094a))
table(children)
```



## Conditional Density Plot (GESIS Panel)

```{r}
cdplot(children ~ age, data = dat)
```



## Binary independent variables with `glm`

-  The [logistic regression](http://data.princeton.edu/R/glms.html) belongs to the class of generalized linear models (GLM)
-  The function for estimating a model of this class in is called `glm()`
-  `glm()` 

### Specifying a `glm`

- formula object 
- the class (binomial, gaussian, gamma) 
- including link function (logit, probit, cauchit, log, cloglog)

must be specified 


## [Logistic regression](http://ww2.coastal.edu/kingw/statistics/R-tutorials/logistic.html) with R

```{r}
glm_1 <- glm(children ~ age, 
                    family = binomial())
```

```{r}
sum_glm1 <- summary(glm_1)
sum_glm1$coefficients
```

<!--
-->

## [Interpreting the coefficients](https://www.sfu.ca/~jackd/Stat302/Wk12-2_Full.pdf)

Consider the logistic model of children in household as a function of age.

```{r}
sum_glm1$coefficients
```

- The estimates and standard errors are given in terms of log 
odds, not in terms of probability.

- The p-values mean the same thing they always have.

## The inverse logit

```{r}
sum_glm1$coefficients
```

```{r,echo=F}
library(faraway)
```


- The coefficients can’t be interpreted as simply as ‘the children in household at age group 0’. 
We have to use the inverse logit in order to find that.

Log-odds of `r sum_glm1$coefficients[1,1]`  is the same as probability 
`r ilogit(sum_glm1$coefficients[1,1])`.

```{r}
library(faraway)
ilogit(sum_glm1$coefficients[1,1])
```

## About the intercept in a logistic model

- It is possible to get an intercept of less than 0.
- This means that the log-odds are negative, NOT the probability.
- E.g. a log-odds of 0 translates to a probability of 
0.5.

## Log-odds and the probability

- Log-odds always increases as probability increases. 

Therefore...

- A positive slope coefficient means that the response increases 
with the associated explanatory variable.

- In this case, the probability of children in the household increases with age.

## Plotting the result



```{r,eval=F,echo=F}
fit_prob <- exp(predict(glm_1))/(1+exp(predict(glm_1)))

library(ggplot2)
dfex <- data.frame(age,fit_prob)
ggplot(aes(x=age, y=fit_prob))  +
  geom_line(aes(x=age, y=fit_prob))
```

but it increases by the sigmoid curve, not at a constant rate.

![](figure/sigmoid.PNG)

<!--
http://www.shizukalab.com/toolkits/plotting-logistic-regression-in-r
-->

## Logistic regression model formula

Logistic models have regression formulas. This model’s formula is:

Log-Odds( Children) =  `r sum_glm1$coefficients[1,1]` + `r sum_glm1$coefficients[2,1]`(Age) + error

We can plug age values into this formula to get predicted log-odds at different ages.

Log-odds for age group 5

```{r,echo=F}
res1 <- sum_glm1$coefficients[1,1] + sum_glm1$coefficients[2,1]*5
```


`r sum_glm1$coefficients[1,1]` + `r sum_glm1$coefficients[2,1]`*(5) =  `r res1`


Children probability in age group 5

```{r}
ilogit(0.3935251)
```


## [Interpreting the results](https://www.r-bloggers.com/how-to-perform-a-logistic-regression-in-r/)

- The difference between the null deviance and the residual deviance shows how our model is doing against the null model (a model with only the intercept). The wider this gap, the better.

```{r}
anova(glm_1, test="Chisq")
```


## [Mc Fadden’s $R^2$](https://www.r-bloggers.com/evaluating-logistic-regression-models/
)

```{r}
library(pscl)
pR2(glm_1) 
```

![](figure/pseudor2.PNG)

<!--
- llh	
The log-likelihood from the fitted model
- llhNull	
The log-likelihood from the intercept-only restricted model
- G2	
Minus two times the difference in the log-likelihoods
- McFadden	
McFadden's pseudo r-squared
- r2ML	
Maximum likelihood pseudo r-squared
- r2CU	
Cragg and Uhler's pseudo r-squared
-->

<!--
## Deviance residuals

- [Pearson VS Deviance Residuals in logistic regression](https://stats.stackexchange.com/questions/166585/pearson-vs-deviance-residuals-in-logistic-regression)

```{r}
summary(sum_glm1$deviance.resid)
```
-->

<!--
https://www.stat.ubc.ca/~rollin/teach/536w10/lec16.pdf
-->
<!--
## Place of living and satisfaction
-->


## Distance between residential area and large city

### How far is it from where you live to the center of the nearest large city?

- 1 - In the center of a big city
- 6 - 60 km and more

```{r}
region <- transform_missings(datf$bczd001a)
table(region)
```

## Satisfaction life in place of residence

### How satisfied are you - all in all - with your life in [place of residence] at the moment?

- 1 - Very satisfied
- 5 - Very dissatisfied

```{r}
satisfactionplace <- datf$a11c019a
table(satisfactionplace)
```


## Another model

```{r,eval=T,echo=T}
glm_2 <- glm(children ~ age + satisfactionplace*region, 
                    family = binomial())
```

```{r,eval=F,echo=F}
# https://www.r-bloggers.com/evaluating-logistic-regression-models/
anova(glm_2, test="Chisq")
```

```{r}
pseudor2 <- pR2(glm_2) 
pseudor2["McFadden"]
```

## Exercise: logistic regression with `Smarket` data

- load the `Smarket` data from the `ISLR` package
- create a `pairs` plot of the data
- check if there are missing values in the data
- have a look at the correltations between variables
- create a `corrplot`   
- run a logistic model and look at the deviance

## Another variable in the Gesis Panel data

- Number of tattoos:

```{r}
Tatoos <- transform_missings(datf$bdao067a)
Tatoos[Tatoos==97]<-0
```

```{r}
table(Tatoos)
```


```{r,eval=F,echo=F}
table(datf$bczd001a)
```


## Generalized regression with R - more functions

- Logistic model with Probit link:

```{r}
probitmod <- glm(children ~ age, 
	family=binomial(link=probit))
```

- Regression with count data:

```{r}
modp <- glm(Tatoos ~ age,family=poisson)
```

- Proportional odds logistic regression in library `MASS`:

```{r}
library("MASS")
mod_plr<-polr(a11c020a ~ a11d096b ,data=dat)
```

<!--
## [B4A Exercise logistic regression](https://www.r-exercises.com/2017/10/30/logistic-regression-in-r/)

- Load the `MASS` package and combine `Pima.tr` and `Pima.tr2` to a data.frame called train and save Pima.te as test. Change the coding of our variable of interest to (type) to 0 (non-diabetic) and 1 (diabetic). Check for and take note of any missing values.

```{r,echo=showsolut}

```
-->

## Exercise: [logistic regression](https://www.r-exercises.com/2017/10/30/logistic-regression-in-r/)

We will use a data on containing health-related measurements on women and whether they can be (or will be at a future point?) classified as diabetic. The data was collected by the US National Institute of Diabetes and is contained in the MASS package.

<!--
Exercise 1
-->

### Load dataset and create test and train dataset

Load the `MASS` package and combine `Pima.tr` and `Pima.tr2` to a `data.frame` called train and save Pima.te as test. Change the coding of our variable of interest to (type) to 0 (non-diabetic) and 1 (diabetic). Check for and take note of any missing values.


<!--
Exercise 2
-->
 
### Plotting using `pairs()` and `jitter`
Take a look at the data. Plot a scatterplot matrix between all the explanatory variables using `pairs()`, and color code the dots according to diabetic classification. Furthermore, try to plot type as a function of age. Use `jitter` to make your graph more informative. Bonus: Can you add a logistic fit based on age on top of your plot?

<!--
Exercise 3
-->

## Exercise: [logistic regression](https://www.r-exercises.com/2017/10/30/logistic-regression-in-r/) II

### Coefficients and p-values
Using the `glm()` and the train data fit a logistic model of type on `age` and `bmi`. Print out the coefficients and their p-value.

<!--
Exercise 4
-->
### prediction

What does the model fitted in exercise 3 predict in terms of probability for someone age 35 with bmi of 32, what about bmi of 22?

<!--
Exercise 5
-->
### Odds ratios
According to our model what are the odds that a woman in our sample is diabetic given age 55 and a bmi 37? Remember that odds in this context have a very precise definition which is different from probability.

<!--
Exercise 6
-->
### confusion matrix

Build the confusion matrix, a table of actual diabetic classification against model prediction. Use a cutoff value of 0.5, meaning that women who the model estimates to have at least 0.5 chance of being diabetic are predicted to be diabetic. What is the prediction accuracy?

<!--
Exercise 7
-->
<!--
## Exercise: [logistic regression](https://www.r-exercises.com/2017/10/30/logistic-regression-in-r/) III


###
Apply the fitted model to the test set. Print the confusion matrix and prediction accuracy.
-->
<!--
Exercise 8
-->
<!--
###
Draw up the ROC curve and calculate the AUC.
-->
<!--
Exercise 9
-->

<!--
###
Add number of pregnancies and age squared as an explanatory variables and redraw the ROC curve on the test set and calculate its AUC.

-->
<!-- 
Exercise 10
-->
<!--
###
For a woman aged 35 and mother of 2 children, by how much does the probability of diabetes increase, if her bmi was 35 instead of 25 according to the model? What about the marginal effect at bmi = 25?
-->

## Linklist - logistic regression

-  Introduction to [**logistic regression**](http://ww2.coastal.edu/kingw/statistics/R-tutorials/logistic.html)

![](figure/Rtutorials.PNG)

-  [**Code for the book of Faraway**](http://www.maths.bath.ac.uk/~jjf23/ELM/scripts/binary.R)

![](figure/orings.PNG)

- [**Categorical data:**](http://homepage.univie.ac.at/herbert.nagel/KategorialeDaten.pdf) - [**How to perform a Logistic Regression in R**](https://www.r-bloggers.com/how-to-perform-a-logistic-regression-in-r/)

- [Logistic Regression in R Tutorial](https://www.datacamp.com/community/tutorials/logistic-regression-R)


<!--
https://www.r-bloggers.com/how-to-perform-a-logistic-regression-in-r/




https://www.datacamp.com/community/tutorials/logistic-regression-R
-->